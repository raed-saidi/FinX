{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ab243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup and Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    print(f\"‚úÖ PyTorch {torch.__version__} loaded\")\n",
    "    print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Installing PyTorch...\")\n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', 'torch'], check=True)\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "print(\"\\n‚úÖ All libraries loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configuration\n",
    "BASE_DIR = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "DATA_DIR = BASE_DIR / \"data\" / \"exported_data\" / \"per_asset\"\n",
    "OUTPUT_DIR = BASE_DIR / \"models\" / \"lstm\"\n",
    "RESULTS_DIR = BASE_DIR / \"results\" / \"lstm\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Assets to train\n",
    "ASSETS = [\"AAPL\", \"NVDA\", \"TSLA\", \"MSFT\", \"GOOGL\", \"AMZN\", \"META\",\n",
    "          \"SPY\", \"QQQ\", \"EFA\", \"IEF\", \"HYG\", \"BIL\", \"INTC\", \"AMD\"]\n",
    "\n",
    "# LSTM Configuration\n",
    "LSTM_CONFIG = {\n",
    "    'sequence_length': 20,      # 20 days of history\n",
    "    'hidden_size': 64,          # LSTM hidden units\n",
    "    'num_layers': 2,            # LSTM layers\n",
    "    'dropout': 0.2,             # Dropout rate\n",
    "    'bidirectional': False,     # Bidirectional LSTM\n",
    "    'attention': True,          # Use attention mechanism\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 50,\n",
    "    'early_stopping_patience': 10\n",
    "}\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üìÇ Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"üñ•Ô∏è Device: {device}\")\n",
    "print(f\"\\n‚öôÔ∏è LSTM Configuration:\")\n",
    "for k, v in LSTM_CONFIG.items():\n",
    "    print(f\"   {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492092b9",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ad0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_asset_data(asset):\n",
    "    \"\"\"Load and combine train/val/test data for an asset.\"\"\"\n",
    "    asset_dir = DATA_DIR / asset\n",
    "    \n",
    "    X_train = pd.read_csv(asset_dir / \"X_train.csv\", index_col=0, parse_dates=True)\n",
    "    X_val = pd.read_csv(asset_dir / \"X_val.csv\", index_col=0, parse_dates=True)\n",
    "    X_test = pd.read_csv(asset_dir / \"X_test.csv\", index_col=0, parse_dates=True)\n",
    "    \n",
    "    y_train = pd.read_csv(asset_dir / \"y_train.csv\", index_col=0, parse_dates=True).squeeze()\n",
    "    y_val = pd.read_csv(asset_dir / \"y_val.csv\", index_col=0, parse_dates=True).squeeze()\n",
    "    y_test = pd.read_csv(asset_dir / \"y_test.csv\", index_col=0, parse_dates=True).squeeze()\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def create_sequences(X, y, seq_length):\n",
    "    \"\"\"Create sequences for LSTM input.\"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    for i in range(len(X) - seq_length):\n",
    "        X_seq.append(X[i:i+seq_length])\n",
    "        y_seq.append(y[i+seq_length])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Test loading\n",
    "asset = 'AAPL'\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = load_asset_data(asset)\n",
    "print(f\"‚úÖ {asset} data loaded:\")\n",
    "print(f\"   Train: {X_train.shape}\")\n",
    "print(f\"   Val: {X_val.shape}\")\n",
    "print(f\"   Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681cfb97",
   "metadata": {},
   "source": [
    "## 4. LSTM Model with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6b2e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention mechanism for sequence models.\"\"\"\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, lstm_output):\n",
    "        # lstm_output: (batch, seq_len, hidden_size)\n",
    "        attention_weights = self.attention(lstm_output)  # (batch, seq_len, 1)\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "        \n",
    "        # Weighted sum\n",
    "        context = torch.sum(attention_weights * lstm_output, dim=1)  # (batch, hidden_size)\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "\n",
    "class LSTMPredictor(nn.Module):\n",
    "    \"\"\"LSTM model for financial time series prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, \n",
    "                 dropout=0.2, bidirectional=False, use_attention=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.use_attention = use_attention\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Output size after LSTM\n",
    "        lstm_output_size = hidden_size * (2 if bidirectional else 1)\n",
    "        \n",
    "        # Attention (optional)\n",
    "        if use_attention:\n",
    "            self.attention = Attention(lstm_output_size)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(lstm_output_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.attention_weights = None  # Store for visualization\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_size)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)  # (batch, seq_len, hidden*directions)\n",
    "        \n",
    "        if self.use_attention:\n",
    "            context, self.attention_weights = self.attention(lstm_out)\n",
    "        else:\n",
    "            # Use last hidden state\n",
    "            context = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Prediction\n",
    "        output = self.fc(context)\n",
    "        return output.squeeze(-1)\n",
    "\n",
    "# Test model\n",
    "n_features = X_train.shape[1]\n",
    "model = LSTMPredictor(\n",
    "    input_size=n_features,\n",
    "    hidden_size=LSTM_CONFIG['hidden_size'],\n",
    "    num_layers=LSTM_CONFIG['num_layers'],\n",
    "    dropout=LSTM_CONFIG['dropout'],\n",
    "    use_attention=LSTM_CONFIG['attention']\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\n‚úÖ LSTM Model created:\")\n",
    "print(f\"   Input features: {n_features}\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4267f8",
   "metadata": {},
   "source": [
    "## 5. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562711b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model(asset, config=LSTM_CONFIG, verbose=True):\n",
    "    \"\"\"Train LSTM model for a single asset.\"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = load_asset_data(asset)\n",
    "    \n",
    "    # Select subset of features for LSTM (top 50 by variance)\n",
    "    feature_var = X_train.var().sort_values(ascending=False)\n",
    "    top_features = feature_var.head(50).index.tolist()\n",
    "    \n",
    "    X_train_sub = X_train[top_features].values\n",
    "    X_val_sub = X_val[top_features].values\n",
    "    X_test_sub = X_test[top_features].values\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_sub)\n",
    "    X_val_scaled = scaler.transform(X_val_sub)\n",
    "    X_test_scaled = scaler.transform(X_test_sub)\n",
    "    \n",
    "    # Create sequences\n",
    "    seq_len = config['sequence_length']\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train.values, seq_len)\n",
    "    X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val.values, seq_len)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test.values, seq_len)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_train_t = torch.FloatTensor(X_train_seq).to(device)\n",
    "    y_train_t = torch.FloatTensor(y_train_seq).to(device)\n",
    "    X_val_t = torch.FloatTensor(X_val_seq).to(device)\n",
    "    y_val_t = torch.FloatTensor(y_val_seq).to(device)\n",
    "    X_test_t = torch.FloatTensor(X_test_seq).to(device)\n",
    "    y_test_t = torch.FloatTensor(y_test_seq).to(device)\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    \n",
    "    # Model\n",
    "    model = LSTMPredictor(\n",
    "        input_size=len(top_features),\n",
    "        hidden_size=config['hidden_size'],\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout=config['dropout'],\n",
    "        use_attention=config['attention']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        train_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val_t)\n",
    "            val_loss = criterion(val_pred, y_val_t).item()\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{config['epochs']}: Train Loss={train_loss:.6f}, Val Loss={val_loss:.6f}\")\n",
    "        \n",
    "        if patience_counter >= config['early_stopping_patience']:\n",
    "            if verbose:\n",
    "                print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_pred = model(X_test_t).cpu().numpy()\n",
    "    \n",
    "    test_true = y_test_seq\n",
    "    \n",
    "    # Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(test_true, test_pred))\n",
    "    mae = mean_absolute_error(test_true, test_pred)\n",
    "    r2 = r2_score(test_true, test_pred)\n",
    "    dir_acc = np.mean(np.sign(test_true) == np.sign(test_pred))\n",
    "    corr = np.corrcoef(test_true, test_pred)[0, 1]\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'features': top_features,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'test_pred': test_pred,\n",
    "        'test_true': test_true,\n",
    "        'metrics': {\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'dir_acc': dir_acc,\n",
    "            'corr': corr\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6a2660",
   "metadata": {},
   "source": [
    "## 6. Train Models for All Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b670f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM for all assets\n",
    "print(\"=\"*70)\n",
    "print(\"LSTM TRAINING - All Assets\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "lstm_results = {}\n",
    "\n",
    "for asset in ASSETS:\n",
    "    print(f\"\\nüîÑ Training {asset}...\")\n",
    "    try:\n",
    "        result = train_lstm_model(asset, verbose=False)\n",
    "        lstm_results[asset] = result\n",
    "        \n",
    "        metrics = result['metrics']\n",
    "        print(f\"  ‚úÖ {asset}: Dir Acc={metrics['dir_acc']:.2%}, Corr={metrics['corr']:.4f}, RMSE={metrics['rmse']:.6f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå {asset}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Trained {len(lstm_results)} LSTM models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc2f074",
   "metadata": {},
   "source": [
    "## 7. Compare LSTM vs XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0d2e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load XGBoost results for comparison\n",
    "xgb_results_path = BASE_DIR / \"results\" / \"xgboost_walkforward\" / \"walkforward_summary.csv\"\n",
    "\n",
    "if xgb_results_path.exists():\n",
    "    xgb_summary = pd.read_csv(xgb_results_path)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison = []\n",
    "    \n",
    "    for asset, result in lstm_results.items():\n",
    "        lstm_metrics = result['metrics']\n",
    "        \n",
    "        # Find XGBoost metrics\n",
    "        xgb_row = xgb_summary[xgb_summary['Asset'] == asset]\n",
    "        if len(xgb_row) > 0:\n",
    "            xgb_metrics = xgb_row.iloc[0]\n",
    "            \n",
    "            comparison.append({\n",
    "                'Asset': asset,\n",
    "                'LSTM_DirAcc': lstm_metrics['dir_acc'],\n",
    "                'XGB_DirAcc': xgb_metrics['Test_DirAcc'],\n",
    "                'LSTM_Corr': lstm_metrics['corr'],\n",
    "                'XGB_Corr': xgb_metrics['Test_Corr'],\n",
    "                'LSTM_RMSE': lstm_metrics['rmse'],\n",
    "                'XGB_RMSE': xgb_metrics['Test_RMSE']\n",
    "            })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL COMPARISON: LSTM vs XGBoost\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüìã Performance by Asset:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nüìä Average Performance:\")\n",
    "    print(f\"   LSTM Dir Accuracy:   {comparison_df['LSTM_DirAcc'].mean():.2%}\")\n",
    "    print(f\"   XGB Dir Accuracy:    {comparison_df['XGB_DirAcc'].mean():.2%}\")\n",
    "    print(f\"   LSTM Correlation:    {comparison_df['LSTM_Corr'].mean():.4f}\")\n",
    "    print(f\"   XGB Correlation:     {comparison_df['XGB_Corr'].mean():.4f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    x = np.arange(len(comparison_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Directional Accuracy\n",
    "    axes[0].bar(x - width/2, comparison_df['LSTM_DirAcc'], width, label='LSTM', color='blue', alpha=0.7)\n",
    "    axes[0].bar(x + width/2, comparison_df['XGB_DirAcc'], width, label='XGBoost', color='green', alpha=0.7)\n",
    "    axes[0].axhline(y=0.5, color='red', linestyle='--', label='Random')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(comparison_df['Asset'], rotation=45)\n",
    "    axes[0].set_ylabel('Directional Accuracy')\n",
    "    axes[0].set_title('Directional Accuracy Comparison')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Correlation\n",
    "    axes[1].bar(x - width/2, comparison_df['LSTM_Corr'], width, label='LSTM', color='blue', alpha=0.7)\n",
    "    axes[1].bar(x + width/2, comparison_df['XGB_Corr'], width, label='XGBoost', color='green', alpha=0.7)\n",
    "    axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(comparison_df['Asset'], rotation=45)\n",
    "    axes[1].set_ylabel('Correlation')\n",
    "    axes[1].set_title('Prediction Correlation Comparison')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Winner count\n",
    "    lstm_wins_dir = (comparison_df['LSTM_DirAcc'] > comparison_df['XGB_DirAcc']).sum()\n",
    "    xgb_wins_dir = (comparison_df['XGB_DirAcc'] > comparison_df['LSTM_DirAcc']).sum()\n",
    "    lstm_wins_corr = (comparison_df['LSTM_Corr'] > comparison_df['XGB_Corr']).sum()\n",
    "    xgb_wins_corr = (comparison_df['XGB_Corr'] > comparison_df['LSTM_Corr']).sum()\n",
    "    \n",
    "    wins = pd.DataFrame({\n",
    "        'Metric': ['Dir Accuracy', 'Correlation'],\n",
    "        'LSTM Wins': [lstm_wins_dir, lstm_wins_corr],\n",
    "        'XGB Wins': [xgb_wins_dir, xgb_wins_corr]\n",
    "    })\n",
    "    \n",
    "    axes[2].bar(['Dir Acc\\nLSTM', 'Dir Acc\\nXGB', 'Corr\\nLSTM', 'Corr\\nXGB'],\n",
    "                [lstm_wins_dir, xgb_wins_dir, lstm_wins_corr, xgb_wins_corr],\n",
    "                color=['blue', 'green', 'blue', 'green'], alpha=0.7)\n",
    "    axes[2].set_ylabel('Number of Assets Won')\n",
    "    axes[2].set_title('Model Wins by Metric')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / 'lstm_vs_xgboost_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    comparison_df.to_csv(RESULTS_DIR / 'lstm_vs_xgboost.csv', index=False)\n",
    "    print(f\"\\n‚úÖ Comparison saved: {RESULTS_DIR / 'lstm_vs_xgboost_comparison.png'}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è XGBoost results not found for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae37c3",
   "metadata": {},
   "source": [
    "## 8. Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43957ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights\n",
    "print(\"üìä Attention Weight Visualization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select an asset with attention\n",
    "viz_asset = 'AAPL'\n",
    "if viz_asset in lstm_results:\n",
    "    result = lstm_results[viz_asset]\n",
    "    model = result['model']\n",
    "    \n",
    "    # Get test data again for visualization\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = load_asset_data(viz_asset)\n",
    "    \n",
    "    # Use same features\n",
    "    X_test_sub = X_test[result['features']].values\n",
    "    X_test_scaled = result['scaler'].transform(X_test_sub)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test.values, LSTM_CONFIG['sequence_length'])\n",
    "    X_test_t = torch.FloatTensor(X_test_seq).to(device)\n",
    "    \n",
    "    # Get attention weights for last batch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model(X_test_t[-10:])  # Last 10 samples\n",
    "    \n",
    "    if model.attention_weights is not None:\n",
    "        attn_weights = model.attention_weights.cpu().numpy()  # (batch, seq_len, 1)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "        \n",
    "        for i, ax in enumerate(axes.flatten()):\n",
    "            if i < len(attn_weights):\n",
    "                weights = attn_weights[i].squeeze()\n",
    "                days = range(-LSTM_CONFIG['sequence_length']+1, 1)\n",
    "                \n",
    "                ax.bar(days, weights, color='blue', alpha=0.7)\n",
    "                ax.set_xlabel('Days Ago')\n",
    "                ax.set_ylabel('Attention Weight')\n",
    "                ax.set_title(f'Sample {i+1}')\n",
    "                ax.set_ylim(0, max(weights)*1.2)\n",
    "        \n",
    "        plt.suptitle(f'{viz_asset} - LSTM Attention Weights\\n(Which past days the model focuses on)', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(RESULTS_DIR / f'{viz_asset}_attention_weights.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Average attention across samples\n",
    "        avg_attention = attn_weights.mean(axis=0).squeeze()\n",
    "        \n",
    "        plt.figure(figsize=(12, 4))\n",
    "        days = range(-LSTM_CONFIG['sequence_length']+1, 1)\n",
    "        plt.bar(days, avg_attention, color='green', alpha=0.7)\n",
    "        plt.xlabel('Days Ago')\n",
    "        plt.ylabel('Average Attention Weight')\n",
    "        plt.title(f'{viz_asset} - Average Attention Pattern\\n(Model focuses more on recent days)')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.savefig(RESULTS_DIR / f'{viz_asset}_avg_attention.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"‚úÖ Attention visualizations saved\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Attention weights not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20833e5f",
   "metadata": {},
   "source": [
    "## 9. Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ffcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LSTM models and results\n",
    "print(\"üíæ Saving LSTM Models and Results...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for asset, result in lstm_results.items():\n",
    "    # Save model\n",
    "    model_path = OUTPUT_DIR / f'{asset}_lstm_model.pt'\n",
    "    torch.save({\n",
    "        'model_state_dict': result['model'].state_dict(),\n",
    "        'features': result['features'],\n",
    "        'config': LSTM_CONFIG\n",
    "    }, model_path)\n",
    "    \n",
    "    # Save scaler\n",
    "    scaler_path = OUTPUT_DIR / f'{asset}_scaler.joblib'\n",
    "    joblib.dump(result['scaler'], scaler_path)\n",
    "    \n",
    "    # Summary\n",
    "    metrics = result['metrics']\n",
    "    summary_data.append({\n",
    "        'Asset': asset,\n",
    "        'Dir_Accuracy': metrics['dir_acc'],\n",
    "        'Correlation': metrics['corr'],\n",
    "        'RMSE': metrics['rmse'],\n",
    "        'MAE': metrics['mae'],\n",
    "        'R2': metrics['r2']\n",
    "    })\n",
    "    \n",
    "    print(f\"  ‚úÖ {asset} saved\")\n",
    "\n",
    "# Save summary\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.to_csv(RESULTS_DIR / 'lstm_summary.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ All models saved to: {OUTPUT_DIR}\")\n",
    "print(f\"‚úÖ Summary saved to: {RESULTS_DIR / 'lstm_summary.csv'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LSTM TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Overall Performance:\")\n",
    "print(f\"   Avg Directional Accuracy: {summary_df['Dir_Accuracy'].mean():.2%}\")\n",
    "print(f\"   Avg Correlation: {summary_df['Correlation'].mean():.4f}\")\n",
    "print(f\"   Avg RMSE: {summary_df['RMSE'].mean():.6f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
